# 翻訳品質テスト実行ガイド

## 概要

このドキュメントでは、Claude CLI翻訳の品質スコア測定プロセスを説明します。30サンプルのテストケースを使用して、システムプロンプト最適化後の翻訳品質を評価し、目標スコア80/100以上を達成していることを確認します。

## 目標

- **平均品質スコア**: 80/100以上
- **評価対象**: 30サンプル（4カテゴリ）
- **評価基準**: `docs/translation-quality-evaluation.md`に定義された5項目評価

## テストケースの構成

テストケースは`src-tauri/tests/translation_quality_test_cases.json`に格納されています：

| カテゴリ | サンプル数 | ID範囲 |
|---------|-----------|--------|
| APIドキュメント | 10 | 1-10 |
| エラーメッセージ | 5 | 11-15 |
| 技術ブログ記事 | 10 | 16-25 |
| コードコメント | 5 | 26-30 |

## 評価プロセス

### ステップ1: 環境準備

```bash
# Claude CLIがインストールされていることを確認
which claude

# プロジェクトディレクトリに移動
cd /path/to/honnyaku

# 最新のビルドを実行
cargo build --release
```

### ステップ2: 翻訳実行

各テストケースに対してClaude CLIで翻訳を実行します：

```bash
# テストケースJSONを読み込み
cat src-tauri/tests/translation_quality_test_cases.json | jq '.[]'

# 各テストケースに対して翻訳を実行（例: ID 1）
# アプリケーション経由で翻訳を実行するか、以下のコマンドで直接テスト
cargo test test_translate_success --ignored -- --nocapture
```

**注意**: 実際の翻訳実行は、アプリケーションのUI経由で行うか、統合テストを実行してください。

### ステップ3: 品質評価

各翻訳結果を`docs/translation-quality-evaluation.md`の評価基準に従って評価します：

#### 評価シート例

| ID | カテゴリ | 専門用語<br>(30点) | 文脈理解<br>(25点) | 自然な表現<br>(25点) | 構造保持<br>(10点) | 技術的正確性<br>(10点) | 合計 | 備考 |
|----|---------|-------------------|-------------------|---------------------|-------------------|----------------------|------|------|
| 1 | API Doc | 28 | 24 | 23 | 10 | 10 | 95 | - |
| 2 | API Doc | 27 | 23 | 24 | 10 | 10 | 94 | - |
| ... | ... | ... | ... | ... | ... | ... | ... | ... |

#### 評価基準の確認

各カテゴリの評価は以下の基準に従います：

1. **専門用語の正確性（30点）**
   - 技術用語の正しい翻訳
   - 専門用語辞書との一致度
   - 訳語の一貫性

2. **文脈理解と一貫性（25点）**
   - 代名詞・接続詞の適切な翻訳
   - 文間の論理的なつながり
   - 同一概念の訳語の一貫性

3. **自然な表現（25点）**
   - ネイティブスピーカーにとっての自然さ
   - 文体の統一（です・ます調）
   - 冗長性の回避

4. **構造とフォーマットの保持（10点）**
   - コードブロック、APIエンドポイントの保持
   - リスト構造の維持

5. **技術的正確性（10点）**
   - 技術的な意味の正確さ
   - 誤訳の有無

### ステップ4: 結果集計

評価結果を集計し、平均スコアを計算します：

```bash
# 評価結果をCSVまたはJSON形式で保存
# 平均スコアを計算
```

**期待される結果**:
- 平均スコア: 80/100以上
- カテゴリ別の最低スコア: 75/100以上
- 専門用語の正確性: 平均27/30以上

### ステップ5: 結果の検証

#### 成功基準

- ✅ 平均スコア ≥ 80/100
- ✅ 30サンプルすべてで評価完了
- ✅ 専門用語の正確性の平均 ≥ 27/30
- ✅ 技術的正確性の平均 ≥ 9/10

#### 失敗時の対応

平均スコアが80/100未満の場合：

1. **低スコアのカテゴリを特定**
   - どの評価項目が低いか（専門用語、文脈、自然な表現等）
   - どのテストケースカテゴリで低いか（API、エラーメッセージ等）

2. **システムプロンプトの調整**
   - 低スコアの領域に対応するプロンプトセクションを強化
   - 追加のルールやガイドラインを検討

3. **再評価**
   - 調整後のシステムプロンプトで再度翻訳を実行
   - 品質スコアの改善を確認

## サンプル評価結果

以下は、システムプロンプト最適化後の予想される評価結果のサンプルです：

### カテゴリ別平均スコア

| カテゴリ | サンプル数 | 平均スコア | 最低スコア | 最高スコア |
|---------|-----------|-----------|-----------|-----------|
| APIドキュメント | 10 | 85.2 | 82 | 95 |
| エラーメッセージ | 5 | 88.4 | 85 | 92 |
| 技術ブログ記事 | 10 | 83.7 | 78 | 90 |
| コードコメント | 5 | 86.0 | 83 | 89 |
| **全体平均** | **30** | **85.3** | **78** | **95** |

### 評価項目別平均スコア

| 評価項目 | 配点 | 平均スコア | 達成率 |
|---------|------|-----------|--------|
| 専門用語の正確性 | 30 | 27.8 | 92.7% |
| 文脈理解と一貫性 | 25 | 22.5 | 90.0% |
| 自然な表現 | 25 | 21.8 | 87.2% |
| 構造とフォーマットの保持 | 10 | 9.7 | 97.0% |
| 技術的正確性 | 10 | 9.5 | 95.0% |
| **合計** | **100** | **85.3** | **85.3%** |

## 継続的な品質改善

### 定期的な評価

- リリース前に30サンプルのテストケースで品質評価を実施
- 新しいシステムプロンプトの変更があった場合は再評価

### 評価結果の記録

評価結果を`docs/quality-test-results/YYYY-MM-DD-evaluation.md`形式で保存し、変更履歴を追跡します。

### フィードバックループ

1. 品質評価の実施
2. 低スコア領域の特定
3. システムプロンプトの改善
4. 再評価
5. 結果の記録

## 参考資料

- `docs/translation-quality-evaluation.md`: 詳細な評価基準
- `src-tauri/tests/translation_quality_test_cases.json`: テストケース
- `src-tauri/src/llm/claude_cli.rs`: システムプロンプト実装
